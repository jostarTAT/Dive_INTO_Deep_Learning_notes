# 6.卷积神经网络

## 6.1.从全连接层到卷积

### 6.1.1.不变性

假设我们想从一张图片中找到某个个体，合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。

卷积神经网络正是将***空间不变性***（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。

将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构：

1. 平移不变性(translation invariance)：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。
2. 局部性(locality)：神经网络前面几层应该只探索输入图像的局部区域，而不过度在意图像中相隔较远区域的关系。

### 6.1.2.多层感知机的限制

重新考察多层感知机

将输入和输出变形为矩阵（宽度、高度）

将权重变形成一个四维的张量

> 这里我的理解是：原本是一个输入的长度到输出的长度的变化，现在是一个输入的高宽变成一个输出的高宽的变化。
>
> 这里的$i,j$代表输出的点在矩阵中的位置，$k,l(或者a,b)$代表输入的点在矩阵中的位置，原本一维时，$w_{i,j}$代表输入点$j$对输出点$i$的影响。而到了二维多层感知机时，$w_{i,j,k,l}$代表的是输入点$k,l$对输出点$i,j$的影响，所以是四维的

则全连接层可以形式化地表示为：
$$
h_{i,j}=\sum_{k,l}w_{i,j,k,l}x_{k,l}=\sum_{a,b}v_{i,j,a,b}x_{i+a,j+b}
$$
$V$是$W$的重新索引，因为这两个四阶张量元素之间存在一一对应关系。我们只需使$k=i+a,l=j+b$，由此可得：$v_{i,j,a,b}=w_{i,j,i+a,j+b}$。索引$a$和$b$通过在正偏移和负偏移之间移动覆盖了整个图像。对于输出中，任意给定位置$(i,j)$处的值$h_{i,j}$，可以通过在$x$中以$(i,j)$为中心对像素加权求和得到，加权使用的权重为$v_{i,j,a,b}$

#### 6.1.2.1.平移不变性

现在引用上述第一个原则：平移不变性。

这意味着检测对象在输入$X$中的平移应该且仅应该导致输出$H$中的平移，而不改变值。也就是说，$V$实际上不依赖于$i,j$的值，即$v_{i,j,a,b}=v_{a,b}$，因此将公式简化为：
$$
h_{i,j} = \sum_{a,b}{v_{a,b}x_{i+a,j+b}}
$$
这就是卷积。我们在使用系数$v_{a,b}$对位置$(i,j)$附近的像素$(i+a,j+b)$进行加权，得到$h_{i,j}$。这一简化使得参数大大减少。

#### 6.1.2.2.局部性

如上所述，为了收集用来训练参数$h_{i,j}$的相关信息，我们不应偏离到距离$(i,j)$很远的地方。这意味着在$|a|>\Delta$或者$|b|>\Delta$的范围之外，我们可以设置$v_{a,b}=0$。因此可以将$h_{i,j}$重写为：
$$
h_{i,j}=\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^{\Delta}v_{a,b}x_{i+a,j+b}
$$
简而言之，这描述的是一个卷积层(convolutional layer)，卷积神经网络是包含卷积层的一种特殊神经网络。

在深度学习社区中，$V$被称之为***卷积核***(convolutional kernel)或者***滤波器***(filter)，亦或很简单的称之为卷积层的权重，通常该权重是可学习的参数。

当图像处理的局部区域很小时，卷积神经网络与多层感知机的训练差异是巨大的。参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。

以上所有的权重学习都将依赖于这种假设。当这种假设与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。但如果这种假设与现实不符，我们的模型可能很难拟合训练数据。

### 6.1.3.卷积

在进一步讨论之前，先简要回顾一下为什么上面的操作被称为卷积。

在数学中，两个函数（比如$f, g: \mathbb{R}^d \to \mathbb{R}$之间的卷积被定义为：
$$
(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.
$$
也就是说，卷积就是把一个函数”翻转“并移位$x$时，测量$f$和$g$之间的重叠。当为离散变量时，积分就变成求和。

例如，对于由索引为$\mathbb{Z}$的、平方可和的、无限维向量集合种抽取的元素，我们得到以下定义：
$$
(f * g)(i) = \sum_a f(a) g(i-a).
$$
对于二维张量，则为$f$的索引$(a,b)$和$g$的索引$(i-a,j-b)$上对应的加和：
$$
(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).
$$
这与上面得到的式子形式类似，主要区别在于：这里不是使用$(i+a,j+b)$而是使用差值。但这种区别是表面的。

我们上面的定义：
$$
h_{i,j}=\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^{\Delta}v_{a,b}x_{i+a,j+b}
$$
更正确地描述了互相关(cross-correlation)。

### 6.1.4.通道

然而这种方法有个问题：我们忽略了一张图片通常包含三个通道/三原色。实际上图片并非二维张量，而是一个由高度、宽度和颜色组成的三维张量。

比如包含$1024 \times 1024 \times 3$个像素，前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。因此，我们将$X$索引为$[X]_{i,j,k}$。由此，卷积相应地调整为$[V]_{a,b,c}$，而不是$[V]_{a,b}$。

此外，由于输入图像是三维的，我们的输出（隐藏表示）$H$也最好采用三维张量。换言之，对于每一个空间位置，我们想采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网络。因此，我们也可以把隐藏表示想象为一系列具有二维张量的通道(channel)，这些通道有时也被称为***特征映射***(feature maps)，因为每个通道都向后续层提供一组空间化的学习特征。直观上可以想象在靠近输入的底层，一些通道专门识别边缘，另一些专门识别纹理。

为支持输入$\mathbf{X}$和隐藏表示$\mathbf{H}$中的多个通道，我们可以在$\mathbf{V}$中添加第四个坐标，即$[\mathbf{V}]_{a,b,c,d}$，综上所述：
$$
[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c},
$$
其中隐藏表示$\mathbf{H}$的索引$d$表示输出通道，而随后的输出将继续以三维张量$\mathbf{H}$作为输入进入下一个卷积层。所以，该式可定义具有多个通道的卷积层，其中$\mathbf{V}$是该卷积层的权重。

> 之前的形式是：
> $$
> h_{i,j}=\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^{\Delta}v_{a,b}x_{i+a,j+b}
> $$
> 这里的$\mathbf{X}$、$\mathbf{H}$都加了一个坐标，是因为多了一个维度，$\mathbf{V}$加了两个坐标，其中一个是因为多了一个输入的维度，另一个是因为选择输出的维度。

### 6.1.5.小结

- 图像平移不变性使我们以相同的方式处理局部图像而不在乎他的位置。
- 局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。
- 在图像处理中，卷积层通常需要比全连接层更少的参数，但仍能获得高效用模型。
- CNN是一类特殊的神经网络，可以包含多个卷积层。
- 多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。



## 6.2.图像卷积

由于卷积神经网络的设计是用于探索图像数据，故本节将会以图像为例。

### 6.2.1.互相关运算

严格讲，卷积层是错误的叫法，他表示的运算实际是互相关运算(cross-correlation)，而不是卷积运算。在卷积层中，输入张量和核张量通过互相关运算产生输出张量。

首先，先忽略通道（第三维），先关注如何处理二维图像数据和隐藏表示。如图，输入是高度为3、宽度为3的二维张量（即形状$3 \times 3$）。卷积核为$2 \times 2$，卷积核窗口（或卷积窗口）的形状由内核的高度和宽度决定（即$2 \times 2$）。

<img src="./Chapter6.assets/image-20250715201427571.png" alt="image-20250715201427571" />



在二维互相关运算中，卷积窗口从输入张量的右上角开始，从左向右、从上到下滑动。当卷积窗口滑动到一个新位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此算出了这一位置的输出张量值。

注意：输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1，而卷积核只与图像中每个大小完全合适的位置做互相关运算。所以输出大小等于输入大小$n_h \times n_\omega$减去卷积核大小$k_h \times k_\omega$，即：
$$
(n_h - k_h+1) \times (n_\omega - k_\omega+1)
$$
接下来，我们在`corr2d`函数中实现该过程，该函数接受输入张量$X$和卷积核张量$K$,并返回输出张量$Y$。

```python
import torch
from torch import nn
def corr2d(X,K):
    h,w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1),(X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i,j] = (X[i:i+h , j:j + w] * K).sum()
    return Y        
```

### 6.2.2.卷积层

卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。

在训练基于卷积层的模型时，我们也随机初始化卷积核权重。

基于上面定义的`corr2d`函数实现二维卷积层，在`__init__`构造函数中，将`weight`和`bias`声明为两个模型参数。前向传播函数调用`corr2d`函数并添加偏置。

```python
class Conv2D(nn.Module):
    def __init__(self,kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))
    def forward(self,x):
        return corr2d(x,self.weight) + self.bias
```

高度和宽度分别为$h$和$w$的卷积核可以被称为$h \times w$卷积或$h \times w$卷积核，我们也将带有$h \times w$卷积核的卷积层称为$h\times w$卷积层。

### 6.2.3.图像中目标的边缘检测

如下是一个卷积层的简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。首先构造一个$6\times 8$像素的黑白图像。中间四列为黑色(0)，其余像素为白色(1)。

```python
X = torch.ones((6,8))
X[:,2:6] = 0
```

接下来，构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为0，否则为非零。

```python
K = torch.tensor([[1.0,-1.0]])
```

现在对参数$X$(输入)和$K$(卷积核)执行互相关运算。输出$Y$中的1代表白色到黑色的边缘，-1表示黑色到白色的边缘，其他情况的输出为0。

```python
Y = corr2d(X,K)
```

```cmd
tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])
```

如果图像转置，则无法检测。

### 6.2.4. 学习卷积核

当有了更复杂数值的卷积核，或者连续的卷积层，手动设计滤波器是不现实的。

现在我们看，是否可以通过仅查看"输入-输出"来学习由X生成Y的卷积核。我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来在每次迭代中，比较Y与卷积层输出的平方误差，然后计算梯度来更新卷积核。简单起见，这里使用内置的二维卷积层并忽略偏置。

```python
# 构造一个二维卷积层，具有一个输出通道和形状为(1,2)的卷积核
conv2d = nn.Conv2d(1,1,kernel_size=(1,2),bias=False)
# 这个二维卷积层使用四维输入和输出格式（批量大小，通道，高度，宽度）
# 批量大小和通道数都为1
X = X.reshape((1,1,6,8))
Y = Y.reshape((1,1,6,7))
lr = 3e-2 
for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y)**2
    conv2d.zero_grad()
    l.sum().backward()
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i+1)%2 == 0:
        print(f"epoch{i+1},loss{l.sum():.3f}")
```

```cmd
epoch 2,loss 11.983
epoch 4,loss 2.914
epoch 6,loss 0.859
epoch 8,loss 0.296
epoch 10,loss 0.112
```

```python
print(conv2d.weight.data.reshape(1,2))
```

```cmd
tensor([[ 0.9726, -0.9929]])
```

与定义的卷积核接近。

### 6.2.5.互相关和卷积

实际上，互相关运算与卷积运算并不相同：
$$
二维交叉相关：\ \ \ \ \ \ \ \
y_{i,j} = \sum_{a=1}^h\sum_{b=1}^{w}w_{a,b}x_{i+a,j+b}\\
二维卷积：   \ \ \ \ \ \ \ \
y_{i,j} = \sum_{a=1}^h\sum_{b=1}^{w}w_{-a,-b}x_{i+a,j+b}
$$
为了得到正式的卷积运算输出，我们需要执行严格定义的卷积运算。但这两者之间差别不大，我们只需水平和垂直翻转二维卷积核张量，然后对输入张量执行互相关运算。

值得注意的是，由于卷积核是从数据中学习到的，因此无论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。

为了说明这一点，假设卷积层执行互相关运算并学习前述的卷积核，该卷积核这里用$K$表示。假设其他条件不变，当这个层执行严格的卷积时，学习的卷积核$K'$在水平和垂直翻转之后将与$K$相同。也就是说，当卷积层对输入和$K'$执行严格卷积运算时，将得到与互相关运算相同的输出。

为了与深度学习文献中的标准术语保持一致，我们将继续把“互相关运算”称为卷积运算。

对于卷积核张量上的权重，我们称其为元素。

### 6.2.6.特征映射和感受野

如6.1.4.节中所述，输出的卷积层有时被称为特征映射，因为它可以被视为一个输入映射到下一层的空间维度的转换器。在CNN中，对于某一层的任意元素$x$，其***感受野***(receptive field)是指***前向传播期间可能影响$x$计算的所有元素(来自所有先前层)***。

感受野可能大于输入的实际大小，继续用该图解释感受野。

![image-20250715201427571](./Chapter6.assets/image-20250715201427571.png)

给定$2\times 2$卷积核，阴影输出元素值19的感受野是输入阴影部分的四个元素。假设之前输出为$Y$，其大小为$2\times 2$，现在我们在其后附加一个卷积层，该卷积层以$Y$为输入，输出单个元素$z$。在这种情况下，$Y$上的$z$的感受野包括$Y$的所有四个元素，而输入的感受野包括最初所有九个输入元素。因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个更深的网络。

### 6.2.7.小结

- 二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关运算，然后添加一个偏置。
- 学习卷积核时，无论使用严格卷积运算或是互相关运算，卷积层的输出不会受太大影响。
- 当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。
- 卷积核大小是超参数。



## 6.3.填充和步幅

假设输入形状为$n_h\times n_w$，卷积核形状为$k_h\times k_w$，则输出形状是$(n_h - k_h+1)\times (n_w-k_w+1)$。因此，卷积的输出形状取决于输入形状和卷积核形状。

有时，在应用连续的卷积后，最终得到的输出大小远小于输入大小，这是由于卷积核的高度、宽度一般大于1。如此一来，原始图像的边界丢失了许多有用的信息，而填充是解决此问题最有效的方法。

有时，我们希望大幅降低图像的宽度和高度，步幅可以在这种情况下提供帮助。

### 6.3.1.填充

如上所述，应用多层卷积时常常丢失边缘像素。由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。但随着我们应用许多连续卷积层，累计丢失的像素数就多了。解决这个问题的简单方法就是***填充***(padding)：在输入图像的边界填充元素（通常填充元素是0）。

如图，将$3\times 3$输入填充到$5\times 5$，那么它的输出就增加为$4\times 4$。

![image-20250716001111496](./Chapter6.assets/image-20250716001111496.png)

通常，如果我们添加$p_h$行填充（大约一半在顶部，一半在底部）和$p_w$列填充（左侧大约一半，右侧大约一半），则输出形状为：
$$
(n_h-k_h+p_h+1)\times (n_w-k_w+p_w+1)
$$
意味着输出的高度和宽度将分别增加$p_h$和$p_w$。

在许多情况下，我们需要设置$p_h=k_h-1$和$p_w=k_w-1$，使输入和输出具有相同的高度和宽度。这样可以在构建网络时更容易预测每个图层的输出形状。

假设$k_h$是奇数，我们将在高度的两侧填充$p_h/2$行。如果$k_h$是偶数，则一种可能性是在输入顶部填充$\lceil p_h/2\rceil$行，在底部填充$\lfloor p_h/2\rfloor$行。填充宽度两侧的规则相同。

卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5、7。选择奇数的好处是，保持空间维度的同时，可以在底部和顶部、左侧和右侧填充相同数量的行/列。

此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量X,当满足：

1. 卷积核的大小是奇数；
2. 所有边的填充行数和列数相同；
3. 输出与输入具有相同高度和宽度

则可以得出：输出$Y[i,j]$是通过以输入$X[i,j]$为中心，与卷积核进行互相关计算得到的。

比如下面的例子中，创建一个高、宽为3的二维卷积层，并在所有侧边填充一个像素。则输入与输出的形状相同。

```python
import torch
from torch import nn
def comp_conv2d(conv2d,X):
    # 这里的(1,1)表示批量大小和通道数都是1
    X = X.reshape((1,1)+X.shape)
    Y = conv2d(X)
    return Y.reshape(Y.shape[2:])
conv2d = nn.Conv2d(1,1,kernel_size = 3,padding = 1)
X = torch.rand(size = (8,8))
comp_conv2d(conv2d,X).shape
```

```cmd
torch.Size([8, 8])
```

当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。如下：

```python
conv2d = nn.Conv2d(1,1,kernel_size=(5,3),padding = (2,1))
```



