# 2.预备知识



## 2.1数据操作

### 2.11 入门

```python
import torch
x = torch.arange(12) 
# 创建一个行向量x，这个行向量包含从0开始的前12个整数
print(x)
# tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
```

可以通过张量的shape属性访问张量的形状。

```python
print(x.shape)
# torch.Size([12])
```

如果只想知道张量的总数，即形状的所有元素乘积，可以检查他的大小（size）。

```python
print(x.numel())
# 12
```

想要改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。

```python
X = x.reshape(3, 4)
print(X)
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11]])
```

我们不需要通过手动指定每个维度来改变形状。 也就是说，如果我们的目标形状是（高度,宽度）， 那么在知道宽度后，高度会被自动计算得出，。 在上面的例子中，为了获得一个3行的矩阵，我们手动指定了它有3行和4列。 幸运的是，我们可以通过`-1`来调用此自动计算出维度的功能。 即我们可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。

```python
Y= x.reshape(-1,4)
Z = x.reshape(3,-1)
print(Y.shape,Z.shape)
# torch.Size([3, 4]) torch.Size([3, 4])
```

有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。代码如下：

全0：

```python
X=torch.zeros(2,3,4)
print(X)
# tensor([[[0., 0., 0., 0.],
#          [0., 0., 0., 0.],
#          [0., 0., 0., 0.]],
#
#         [[0., 0., 0., 0.],
#          [0., 0., 0., 0.],
#          [0., 0., 0., 0.]]])
```

全1：

```python
Y=torch.ones(2,3,4)
print(Y)
```

有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。*** 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样***。

```python
Z=torch.randn(3, 4)
print(Z)
# tensor([[ 1.3945, -1.3954, -0.1719, -0.8006],
#         [-0.9903,  1.2498,  0.8310, -1.7149],
#         [-1.6048, -0.5111, -2.6971,  0.3204]])
```

我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1。

```python
q=torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(q)
# tensor([[2, 1, 4, 3],
#         [1, 2, 3, 4],
#         [4, 3, 2, 1]])
```

### 2.1.2运算符

对于任意具有相同形状的张量， 常见的标准算术运算符（`+`、`-`、`*`、`/`和`**`（求幂））都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。

```python
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
print(x + y, x - y, x * y, x / y, x ** y)
# (tensor([ 3.,  4.,  6., 10.]),
#  tensor([-1.,  0.,  2.,  6.]),
#  tensor([ 2.,  4.,  8., 16.]),
#  tensor([0.5000, 1.0000, 2.0000, 4.0000]),
#  tensor([ 1.,  4., 16., 64.]))
```

“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。

```python
torch.exp(x)
# tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
```

除了按元素计算外，我们还可以执行线性代数运算，包括向量点积和矩阵乘法。在2.3节中解释。

我们也可以把多个张量*连结*（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。 下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。 我们可以看到，第一个输出张量的轴-0长度（6）是两个输入张量轴-0长度的总和（3+3）； 第二个输出张量的轴-1长度（8）是两个输入张量轴-1长度的总和（4+4）。

```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(X,'\n',Y)
print(torch.cat((X, Y), dim=0),"\n", torch.cat((X, Y), dim=1))
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11.]]) 
#  tensor([[2., 1., 4., 3.],
#         [1., 2., 3., 4.],
#         [4., 3., 2., 1.]])
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11.],
#         [ 2.,  1.,  4.,  3.],
#         [ 1.,  2.,  3.,  4.],
#         [ 4.,  3.,  2.,  1.]]) 
# tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
#         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
#         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])
```

有时，我们想通过*逻辑运算符*构建二元张量。 以`X == Y`为例： 对于每个位置，如果`X`和`Y`在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句`X == Y`在该位置处为真，否则该位置为0。

```python
print(X==Y)
# tensor([[False,  True, False,  True],
#         [False, False, False, False],
#         [False, False, False, False]])
```

对张量中的所有元素进行求和，会产生一个单元素张量。

```python
print(X.sum())
# tensor(66.)
```

### 2.1.3广播机制

在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。

在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
print(a,'\n', b)
# tensor([[0],
#         [1],
#         [2]]) 
#  tensor([[0, 1]])

```

由于`a`和`b`分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵*广播*为一个更大的3×2矩阵，如下所示：矩阵`a`将复制列， 矩阵`b`将复制行，然后再按元素相加。

```python
print(a+b)
# tensor([[0, 1],
#         [1, 2],
#         [2, 3]])
```

### 2.1.4索引与切片

张量中的元素可以通过索引访问。第一个元素的索引是0，最后一个元素的索引是-1。

我们可以用`[-1]`选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素：

```python
print(X[-1],'\n',X[1:3])
# tensor([ 8.,  9., 10., 11.]) 
# tensor([[ 4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11.]])
```

除读取外，我们还可以通过指定索引来将元素写入矩阵

```python
X[1, 2] = 9
print(X)
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5.,  9.,  7.],
#         [ 8.,  9., 10., 11.]])
```

如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，`[0:2, :]`访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。 虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。

```python
X[0:2, :] = 12
print(X)
# tensor([[12., 12., 12., 12.],
#         [12., 12., 12., 12.],
#         [ 8.,  9., 10., 11.]])
```

### 2.1.5节省内存

运行一些操作可能会导致为新结果分配内存。 例如，如果我们用`Y = X + Y`，我们将取消引用`Y`指向的张量，而是指向新分配的内存处的张量。

在下面的例子中，我们用Python的`id()`函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。 这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。

```python
before = id(Y)
Y = Y + X
print(id(Y) == before)
# False
```

这可能是不可取的，原因有两个：

1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。 为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同， 使用`zeros_like`来分配一个全0的块。

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
# id(Z): 2184907389984
# id(Z): 2184907389984
```

### 2.1.6转换为其他Python对象

将深度学习框架定义的张量转换为NumPy张量（`ndarray`）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。

```python
A = X.numpy()
B = torch.tensor(A)
print(type(A), type(B))
# <class 'numpy.ndarray'> <class 'torch.Tensor'>
```

要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数。

```python
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
# (tensor([3.5000]), 3.5, 3.5, 3)
```

## 2.2数据预处理

本节我们将简要介绍使用`pandas`预处理原始数据，并将原始数据转换为张量格式的步骤。

### 2.2.1读取数据集

首先创建一个人工数据集，并存储在CSV（逗号分隔值）文件 `../data/house_tiny.csv`中。 以其他格式存储的数据也可以通过类似的方式进行处理。 下面我们将数据集按行写入CSV文件中。

```python
import os

os.makedirs(os.path.join('..', 'data'), exist_ok=True) 
#".."表示上级目录，exist_ok=True让makedirs在目标文件夹已经存在时不会报错。
data_file = os.path.join('..', 'data', 'house_tiny.csv') #data_file = ../data/house_tiny.csv
with open(data_file, 'w') as f:
    f.write('NumRooms,Alley,Price\n')  # 列名
    f.write('NA,Pave,127500\n')  # 每行表示一个数据样本
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
```

```python
import pandas as pd
data = pd.read_csv(data_file)
print(data)
#    NumRooms Alley   Price
# 0       NaN  Pave  127500
# 1       2.0   NaN  106000
# 2       4.0   NaN  178100
# 3       NaN   NaN  140000
```

### 2.2.2处理缺失值

注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括*插值法*和*删除法*， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在这里，我们将考虑插值法。

通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`， 其中前者为`data`的前两列，而后者为`data`的最后一列。 

对于`inputs`中缺少的`数值`，我们用同一列的均值替换“NaN”项。

```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
print(inputs)
#    NumRooms Alley
# 0       3.0  Pave
# 1       2.0   NaN
# 2       4.0   NaN
# 3       3.0   NaN
```

对于`类别值`或`离散值`，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， `pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。

```python
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
#    NumRooms  Alley_Pave  Alley_nan
# 0       3.0           1          0
# 1       2.0           0          1
# 2       4.0           0          1
# 3       3.0           0          1
```

> dummy_na的作用是让值为NaN的单独成列。

### 2.2.3转换为张量格式

现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式。

```python
X = torch.tensor(inputs.to_numpy(dtype=float))
y = torch.tensor(outputs.to_numpy(dtype=float))
print(X,'\n', y)
# tensor([[3., 1., 0.],
#         [2., 0., 1.],
#         [4., 0., 1.],
#         [3., 0., 1.]], dtype=torch.float64) 
# tensor([127500., 106000., 178100., 140000.], dtype=torch.float64)
```

## 2.3线性代数

### 2.3.1标量

标量由只有一个元素的张量表示。

```python
x = torch.tensor(3.0)
y = torch.tensor(2.0)
```

### 2.3.2向量

向量可以被视为标量值组成的列表。 这些标量值被称为向量的*元素*（element）或*分量*（component）。

通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。

```python
x = torch.arange(4)
```

我们可以使用下标来引用向量的任一元素，例如可以通过x~i~来引用第i个元素。

列向量是向量的默认方向。

#### 2.3.2.1长度、维度和形状

向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。 在数学表示法中，如果我们想说一个向量x由n个实值标量组成， 可以将其表示为x∈R^n^。 向量的长度通常称为向量的*维度*（dimension）。

与普通的Python数组一样，我们可以通过调用Python的内置`len()`函数来访问张量的长度。

```python
len(x)
```

当用张量表示一个向量（只有一个轴）时，我们也可以通过`.shape`属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。

```python
x.shape
# torch.Size([4])
```

### 2.3.3矩阵

正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。 矩阵，我们通常用粗体、大写字母来表示 （例如，**X**、**Y**和**Z**）， 在代码中表示为具有两个轴的张量。

数学表示法使用A∈R^m×n^ 来表示矩阵A，其由m行和n列的实值标量组成。 我们可以将任意矩阵A∈R^m×n^视为一个表格， 其中每个元素a~ij~属于第i行第j列。

对于任意A∈R^m×n^， A的形状是（m,n）或m×n。 当矩阵具有相同数量的行和列时，其形状将变为正方形； 因此，它被称为*方阵*（square matrix）。

当调用函数来实例化张量时， 我们可以通过指定两个分量m和n来创建一个形状为m×n的矩阵。

```python
A = torch.arange(20).reshape(5, 4)
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11],
#         [12, 13, 14, 15],
#         [16, 17, 18, 19]])
```

当我们交换矩阵的行和列时，结果称为矩阵的*转置*（transpose）。 通常用a^⊤^来表示矩阵的转置，如果B=A^⊤^， 则对于任意i和j，都有b~ij~=a~ji~。 

现在在代码中访问矩阵的转置。

```python
A.T
```

作为方阵的一种特殊类型，*对称矩阵*（symmetric matrix）A等于其转置：A=A^⊤^。 

尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中， 将每个数据样本作为矩阵中的行向量更为常见。

### 2.3.4张量

就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的n维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。张量的索引机制（例如X~ijk~）与矩阵类似。

当我们开始处理图像时，张量将变得更加重要，图像以n维数组形式出现， 其中3个轴对应于高度、宽度，以及一个*通道*（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。 现在先将高阶张量暂放一边，而是专注学习其基础知识。

```python
X = torch.arange(24).reshape(2, 3, 4)
```

### 2.3.5张量算法的基本性质

标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。 例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。 例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
```

具体而言，两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号⊙）。

```python
A * B
```

将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。

```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
X*a
```

### 2.3.6降维

我们可以对任意张量进行的一个有用的操作是计算其元素的和。

```python
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```

默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print(A)
A_sum_axis0 = A.sum(axis=0)
print(A_sum_axis0)
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11.],
#         [12., 13., 14., 15.],
#         [16., 17., 18., 19.]])
# tensor([40., 45., 50., 55.])
```

> 我对于沿axis=0的理解是，轴0是行，运算就是∑A[ i ] [ j ], j相同，最后的结果就是沿着上下（列）求和。

指定`axis=1`将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。

```python
A_sum_axis1 = A.sum(axis=1)
print(A_sum_axis1)
# tensor([ 6., 22., 38., 54., 70.])
```

沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。

```python
A.sum(axis=[0, 1])  # 结果和A.sum()相同
```

一个与求和相关的量是*平均值*（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。

```python
A.mean(), A.sum() / A.numel() #这两种效果相同
# (tensor(9.5000), tensor(9.5000))
```

同样，计算平均值的函数也可以沿指定轴降低张量的维度。

```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))
```

#### 2.3.6.1非降维求和

```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```

由于`sum_A`在对每行进行求和后仍保持两个轴，我们可以通过广播将`A`除以`sum_A`。

```python
A / sum_A
# tensor([[0.0000, 0.1667, 0.3333, 0.5000],
#         [0.1818, 0.2273, 0.2727, 0.3182],
#         [0.2105, 0.2368, 0.2632, 0.2895],
#         [0.2222, 0.2407, 0.2593, 0.2778],
#         [0.2286, 0.2429, 0.2571, 0.2714]])
```

如果我们想沿某个轴计算`A`元素的累积总和， 比如`axis=0`（按行计算），可以调用`cumsum`函数。 此函数不会沿任何轴降低输入张量的维度。

```python
A.cumsum(axis=0)
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  6.,  8., 10.],
#         [12., 15., 18., 21.],
#         [24., 28., 32., 36.],
#         [40., 45., 50., 55.]])
```

> 这里是累积总和，比如cumsum[1] [0] = A[0] [0]+A[1] [0]

### 2.3.7 点积（Dot Product)

向量的点积

```python
y = torch.ones(4, dtype = torch.float32)
torch.dot(x, y)
# tensor(6.)
```

对于向量来说，与下式相同。

```python
torch.sum(x * y)
```

### 2.3.8矩阵-向量积

在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。 当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积。 注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。

```python
A.shape, x.shape, torch.mv(A, x)
# (torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))
```

## 2.5自动微分

深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

### 2.5.1一个简单的例子

作为一个演示例子，假设我们想对函数y=2x^⊤^x关于列向量x求导。 首先，我们创建变量`x`并为其分配一个初始值

```python
x = torch.arange(4.0)
```

在我们计算y关于x的梯度之前，需要一个地方来存储梯度。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。

```python
x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad  # 默认值是None
```

```python
y = 2 * torch.dot(x, x)
print(y)
# tensor(28., grad_fn=<MulBackward0>)
```

`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。 接下来，通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度，并打印这些梯度。

```python
y.backward()
print(x.grad)
# tensor([ 0.,  4.,  8., 12.])
```

函数y=2x^⊤^x关于x的梯度应为4x。 让我们快速验证这个梯度是否计算正确。

```python
x.grad == 4 * x
# tensor([True, True, True, True])
```

现在计算`x`的另一个函数。

```python
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()
y = x.sum()
y.backward()
print(x.grad)
# tensor([1., 1., 1., 1.])
```

### 2.5.2非标量变量的反向传播

当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。 对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。

然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。

```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```

### 2.5.3分离计算

有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。 想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数， 并且只考虑到`x`在`y`被计算后发挥的作用。

这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。 换句话说，梯度不会向后流经`u`到`x`。 因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理， 而不是`z=x*x*x`关于`x`的偏导数。

```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u  # tensor([True, True, True, True])
```

# 3.线性神经网络

## 3.1线性回归

### 3.1.1线性回归的基本元素

#### 3.1.1.1线性模型

线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子：
$$
\text{price} = w_{\text{area}} \cdot \text{area} + w_{\text{age}} \cdot \text{age} + b
$$
$w_{\text{area}}$ 和$w_{\text{age}}$称为权重(weight),权重决定了每个特征对于我们预测值的影响。

b称为偏置(bias)、偏移量(offset)或截距(intercept。偏置是指当所有特征都取值为0时，预测值应该为多少。如果没有偏置项，模型的表达能力将受到限制。

严格来说，上述公式是输入特征的一个 *仿射变换*（affine transformation）。 仿射变换的特点是通过加权和对特征进行*线性变换*（linear transformation）， 并通过偏置项来进行*平移*（translation）。

给定一个数据集，我们的目标是寻找模型的权重w和偏置b， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过*线性模型*的仿射变换决定，仿射变换由所选权重和偏置确定。

而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含d个特征时，我们将预测结果 $\hat{y}$（通常使用“尖角”符号表示y的估计值）表示为：
$$
\hat{y} = w_1 x_1 + \cdots + w_d x_d + b.
$$
将所有特征放到向量x∈R^d^中， 并将所有权重放到向量w∈R^d^中， 我们可以用点积形式来简洁地表达模型：
$$
\hat{y} = \mathbf{w}^T \mathbf{x} + b.
$$
向量x对应于单个数据样本的特征。 用符号表示的矩阵X∈R^n×d^ 可以很方便地引用我们整个数据集的n个样本。 其中，X的每一行是一个样本，每一列是一种特征。

对于特征集合X，预测值$\hat{y}$∈R^n^ 可以通过矩阵-向量乘法表示为：
$$
\hat{y} = Xw + b
$$
这个过程的求和使用广播机制。

给定训练数据特征X和对应的已知标签y， 线性回归的目标是找到一组权重向量w和偏置b： 当给定从X的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。

虽然我们相信给定x预测y的最佳模型会是线性的， 但我们很难找到一个有n个样本的真实数据集，其中对于所有的1≤i≤n，y(i)完全等于w^⊤^x^(i)^+b。 无论我们使用什么手段来观察特征X和标签y， 都可能会出现少量的观测误差。 因此，即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。

在开始寻找最好的*模型参数*（model parameters）w和b之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。

#### 3.1.1.2损失函数

损失函数(loss function)能够量化目标的实际值与预测值之间的差距。通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时损失为0。

回归问题中最常用的损失函数是平方误差函数。$\hat{y}^{(i)}$表示样本i的预测值，${y}^{(i)}$表示相应的真实标签。平方误差可定义为：
$$
l^{(i)}(w, b) = \frac{1}{2} (\hat{y}^{(i)} - y^{(i)})^2.
$$
常数$\frac{1}{2}$不会带来本质的差别，但这样在形式上稍微简单一些 （因为对损失函数求导后常数系数为1）。

为了度量模型在整个数据集上的质量，计算在训练集n个样本上的损失均值：
$$
L(w, b) = \frac{1}{n} \sum_{i=1}^{n} l^{(i)}(w, b) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} (w^T x^{(i)} + b - y^{(i)})^2.
$$
训练模型时，希望找到一组参数(w^*^,b^*^)，能够使L(w,b)最小，即：
$$
w^*, b^* = \arg\min_{w,b} L(w, b).
$$

#### 3.1.1.3解析解

线性回归的解可以用一个公式简单的表示出来，这类解叫做解析解。

首先，我们将偏置b合并到参数w中，合并方法是在包含所有参数的矩阵中附加一列。

省去推导过程，解为： 
$$
w^* = (X^T X)^{-1} X^T y.
$$
但并非所有问题都存在解析解，导致它无法广泛应用在深度学习中。

#### 3.1.1.4随机梯度下降

在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（这里也可称之为梯度）。但实际中执行可能会非常慢，因为每次更新参数前必须遍历整个数据集，因此通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做**小批量随机梯度下降**（minibatch stochastic gradient descent）

每次迭代时，首先随机抽样一个小批量的$\mathcal{B}$,它由***固定数量***的训练样本组成。然后计算小批量的平均损失关于模型参数的导数（梯度）。最后将梯度乘以一个预先确定的正数η，并从当前参数的值中减掉。

用数学公式表示为：（$\partial$表示偏导数）
$$
(\mathbf{w}, b) \gets (\mathbf{w}, b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w}, b)} l^{(i)} (\mathbf{w}, b).
$$

> 算法步骤如下：
>
> 1. 初始化模型参数的值。
> 2. 从数据集中随机抽取小批量样本并且在负梯度方向上更新参数，不断迭代这一步骤。

$|\mathcal{B}|$表示每个小批量中的样本数，也成为批量大小(batch size)。η称为学习率(learning rate)。批量大小和学习率是手动预先指定的。这些可以调整但不在训练过程中更新的参数称为超参数(hyperparameter)。

调参(hyperparameter tuning)是选择超参数的过程。超参数通常是根据训练迭代训练迭代结果来调整的，而训练迭代结果是在独立的验证数据集(validation dataset)上评估得到的。

训练了预先确定的若干迭代次数后（或满足某些其他条件后），我们记录下模型参数的估计值，表示为$\hat{w},\hat{b}$。但即使函数确实是线性且无噪声的，这些估计值也不会使损失函数真正得到最小值。因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。

线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在*训练集*上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为*泛化*（generalization）。

#### 3.1.1.5用模型进行预测

给定“已学习”的线性回归模型$\mathbf{\hat{w}}^T \mathbf{x} + \hat{b}$，现在我们可以通过房屋面积x~1~和房龄x~2~来估计一个（未包含在训练数据中的）新房屋价格。 给定特征估计目标的过程通常称为*预测*（prediction）或*推断*（inference）。

### 3.1.2矢量化加速

在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

为了说明矢量化为什么如此重要，我们考虑对向量相加的两种方法。 我们实例化两个全为1的10000维向量。 在一种方法中，我们将使用Python的for循环遍历向量； 在另一种方法中，我们将依赖对`+`的调用。

```python
n = 10000
a = torch.ones([n])
b = torch.ones([n])
```

具体测试代码省略，可以观察到，使用重载的+运算符来计算按元素的和远快于使用for循环每次执行一位加法。

矢量化代码通常会带来数量级的加速。 另外，我们将更多的数学运算放到库中，而无须自己编写那么多的计算，从而减少了出错的可能性。

### 3.1.3正态分布与平方损失

接下来通过对噪声分布的假设来解读平方损失目标函数。

正态分布和线性回归之间的关系很密切。 正态分布（normal distribution），也称为*高斯分布*（Gaussian distribution）， 简单的说，若随机变量x具有均值μ和方差σ^2^（标准差σ），其正态分布概率密度函数如下：
$$
p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{1}{2\sigma^2} (x - \mu)^2 \right).

$$
改变均值会产生沿x轴的偏移，增加方差将会分散分布、降低峰值。

==***后面的看不懂***==

### 3.1.4从线性回归到深度网络

#### 3.1.4.1

在下图中，我们将线性回归模型描述为一个神经网络，该图只显示连接模式，隐去了权重和偏置的值。

![image-20250312163421086](C:\Users\Ai\AppData\Roaming\Typora\typora-user-images\image-20250312163421086.png)

如图所示的神经网络中，输入为x~1~,x~2~,...,x~d~，因此输入层中的输入数（或称特征维度，feature dimensionality）为d。网络输出为o~1~，因此输出层中的输出数是1。

需要注意的是，输入值都是已给定的，且只有一个计算神经元。我们通常在计算层数时不考虑输入层，即该图中神经网络的层数为1。我们可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为单层神经网络。

对于线性回归，每个输入都与每个输出相连，这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）。

## 3.2线性回归的从零开始实现。

> 这一节将从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。 同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。 在这一节中，我们将只使用张量和自动求导。 在之后的章节中，我们会充分利用深度学习框架的优势，介绍更简洁的实现方式。

```python
import random
import torch
from d2l import torch as d2l
```

### 3.2.1生成数据集

为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。 我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 我们将使用低维数据，这样可以很容易地将其可视化。 在下面的代码中，我们生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。 我们的合成数据集是一个矩阵X∈R^1000×2^。

我们将使用线性模型参数 w = [2,-3,4]^T^、b = 4.2和噪声项$\epsilon$生成数据集及其标签：
$$
y = Xw + b + \epsilon.
$$
$\epsilon$可以视为模型预测和标签时的潜在观测误差。在这里我们认为标准假设成立，即ϵ服从均值为0的正态分布。 为了简化问题，我们将标准差设为0.01。 下面的代码生成合成数据集。

```python
def synthetic_data(w, b, num_examples):  #@save
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w))) #num_examples x len(w)
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1)) #变成 ? x 1

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

> ```python
> torch.normal(mean, std, size, *, generator=None, out=None)
> ```
>
> `torch.normal()` 用于从**正态分布（高斯分布）**中生成随机数，并以张量的形式返回。
>
> mean:均值（可以是单个数值或是张量）
>
> std:标准差（可以说单个数值或是张量）
>
> size()：生成的张量形状（仅在mean和std是标量时使用）
>
> ```python
> torch.matmul(input, other, *, out=None) → Tensor
> ```
>
> `torch.matmul()` 主要用于矩阵乘法（Matrix Multiplication），它支持 **向量-向量、矩阵-向量、矩阵-矩阵** 等不同类型的乘法运算。它可以自动根据输入张量的维度调整运算方式。
>
> | 参数    | 说明                           |
> | ------- | ------------------------------ |
> | `input` | 第一个张量（可以是向量或矩阵） |
> | `other` | 第二个张量（可以是向量或矩阵） |
> | `out`   | 可选参数，指定存放结果的张量   |
>
> 1. 向量与向量相乘：点积
> 2. 矩阵与向量相乘：矩阵-向量乘法
> 3. 矩阵与矩阵相乘：矩阵乘法

注意，`features`中的每一行都包含一个二维数据样本， `labels`中的每一行都包含一维标签值（一个标量）。

### 3.2.2读取数据集





























